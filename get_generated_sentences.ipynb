{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b43b61-b8b1-475f-83c3-21c3c448e54a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os,pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BitsAndBytesConfig,LlamaTokenizer,LlamaForCausalLM \n",
    "from Custom_bert_generate2 import CustomDataset, Model,get_generated_sentence_data, get_accuracy_by_bert\n",
    "from get_model import get_model\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.get_keywords import get_keywords_from_reference\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import transformers\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "device = 'cuda'\n",
    "model_names = ['LLaMA-2-7B-Chat','LLaMA-3-7B-Instruct','Alpaca-7B',\n",
    "               'DialoGPT-large', 'GPT-2-large',\n",
    "               'WizardLM-2-7B','WizardLM-7B','WizardCoder-7B','WizardMath-7B',\n",
    "              'ChatGLM-6B','Guanaco-2-7B']\n",
    "model_name = model_names[8]\n",
    "\n",
    "tokenizer,model = get_model(model_name,device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09f4b9a-c22d-46ca-8c9e-0a82ba0dd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with open('/home/wooseok/peft/FT_input_datasets_llama.pickle', 'rb') as fr:\n",
    "        data = pickle.load(fr)\n",
    "    del_index = []\n",
    "    for i in range(len(data)):\n",
    "        if len(data['context'][i]) %2 ==0:\n",
    "            del_index.append(i)\n",
    "    data.drop(del_index, axis=0, inplace=True)\n",
    "    data=data.reset_index(drop = True)\n",
    "    return data\n",
    "\n",
    "if model_name == 'Guanaco-2-7B':\n",
    "    def make_prompt(j,attribute,sys_prompt,data):\n",
    "        default_prompt = \"### Instruction:\"+\"\\n\"\n",
    "        sys_prompt = default_prompt+ sys_prompt + attribute+\"\\n\"\n",
    "        prompt = sys_prompt \n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            if i%2 == 0:\n",
    "                utter2 = 'USER: '+data['context'][j][i]+\"\\n\"\n",
    "            if i%2 == 1:\n",
    "                utter2 = 'ASSISTANT: '+data['context'][j][i]\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = prompt+\"### Input: \"+\"\\n\"+conversation_history+\"\\n### Response:\" +\"\\nASSISTANT: \"\n",
    "        return prompt, conversation_history\n",
    "if model_name in ['WizardLM-2-7B','WizardLM-7B']:\n",
    "    def make_prompt(j,attribute,sys_prompt,data):\n",
    "        default_prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "        sys_prompt = default_prompt+ sys_prompt + attribute\n",
    "        prompt = sys_prompt \n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            if i%2 == 0:\n",
    "                utter2 = 'USER: '+data['context'][j][i]\n",
    "            if i%2 == 1:\n",
    "                utter2 = 'ASSISTANT: '+data['context'][j][i]+\"</s>\"\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = prompt+conversation_history +\"ASSISTANT: \"\n",
    "        return prompt, conversation_history\n",
    "       \n",
    "if model_name in ['DialoGPT-large', 'GPT-2-large']:\n",
    "    def make_prompt(j,attribute,sys_prompt,data):\n",
    "        sys_prompt = sys_prompt +\"\\n\" + attribute+\"<|endoftext|>\"+\"\\n\"\n",
    "        utter1 = data['context'][j][0] \n",
    "        prompt = sys_prompt +'Conversation:'\n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            utter2 = '\\n'+data['context'][j][i] +\"\\n\"\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = prompt+conversation_history+\"<|endoftext|>\"\n",
    "        return prompt, conversation_history\n",
    "\n",
    "if model_name == 'LLaMA-3-7B-Instruct':\n",
    "\n",
    "    def make_prompt(j,attribute,sys_prompt,data):\n",
    "        sys_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"+sys_prompt +attribute\n",
    "        sys_prompt = sys_prompt +\"<|eot_id|><|start_header_id|>user1<|end_header_id|>\\n\"\n",
    "        \n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            if i%2 == 0:\n",
    "                utter2 = '\\n\\n'+data['context'][j][i] +'<|eot_id|><|start_header_id|>user2<|end_header_id|>'\n",
    "            if i%2 == 1:\n",
    "                utter2 = '\\n\\n'+data['context'][j][i] +'<|eot_id|><|start_header_id|>user1<|end_header_id|>'\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = sys_prompt+conversation_history\n",
    "        \n",
    "        return prompt, conversation_history\n",
    "if model_name == 'LLaMA-2-7B-Chat':\n",
    "    def make_prompt(j,attribute,sys_prompt,data):    \n",
    "        sys_prompt = \"[INST]<<SYS>>\\n\"+sys_prompt +\"\\n<</SYS>>\\n\" + attribute+\"\\n\"+'Conversation:'\n",
    "         \n",
    "        \n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            utter2 = '\\n'+data['context'][j][i] +'</s>'\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = sys_prompt+conversation_history +'[/INST]'\n",
    "        return prompt, conversation_history\n",
    "if model_name in ['Alpaca-7B','WizardCoder-7B','WizardMath-7B','ChatGLM-6B']:\n",
    "    def make_prompt(j,attribute,sys_prompt,data):\n",
    "        default_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "\n",
    "        default_prompt = default_prompt +\"\\n### Instruction:\\n\"\n",
    "        sys_prompt = default_prompt + sys_prompt + attribute +\"\\n\"+\"\\n### Input:\"\n",
    "        \n",
    "        prompt = sys_prompt\n",
    "        conversation_history = ''\n",
    "        for i in range(len(data['context'][j])):\n",
    "            if i%2 == 0:\n",
    "                utter2 = '\\n'+data['context'][j][i] +'</s>'\n",
    "            if i%2 == 1:\n",
    "                utter2 = '\\n'+data['context'][j][i] +'</s>'\n",
    "            conversation_history = conversation_history + utter2\n",
    "        prompt = prompt+conversation_history +\"\\n\\n### Response:\"\n",
    "        \n",
    "        return prompt, conversation_history\n",
    "def prompt_make(attribute,sys_prompt):\n",
    "    data = get_data()\n",
    "    \n",
    "    prompt_gen = []\n",
    "    history_gen = []\n",
    "    for j in tqdm(range(len(data))):\n",
    "        kk = make_prompt(j,attribute,sys_prompt,data)\n",
    "        prompt_gen.append(kk[0])\n",
    "        history_gen.append(kk[1])\n",
    "                \n",
    "    data['prompt']=prompt_gen\n",
    "    data['history']=history_gen\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67939144-90d5-4739-93c6-f9c3d40a4609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_generated_sentences_data(attribute, sys_prompt,data_num,emotion,input_label):\n",
    "    \n",
    "    data = prompt_make(attribute,sys_prompt)\n",
    "    data = data.sample(frac=1,random_state = 2).reset_index(drop=True)[:data_num]\n",
    "    generated_sentences_masking_attribute = []\n",
    "    generated_sentences = []\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        print(data['prompt'][i])\n",
    "        \n",
    "        encoded_context = torch.tensor([tokenizer.encode(data['prompt'][i],add_special_tokens= False)])\n",
    "        eeencoded_context = encoded_context.squeeze()[:-1].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(eeencoded_context, output_hidden_states=True,use_cache=True)\n",
    "        past_key_values = outputs['past_key_values']\n",
    "        \n",
    "        past_length = past_key_values[0][0].shape[2]\n",
    "        eencoded_context = encoded_context.squeeze()[-1].unsqueeze(0).unsqueeze(0)\n",
    "        attention_mask = torch.ones(1, past_length + len(eencoded_context[0]), dtype=torch.long)\n",
    "        generated_ids = model.generate(eencoded_context.to(device),\n",
    "                                       attention_mask=attention_mask.to(device),\n",
    "                                       past_key_values=past_key_values,\n",
    "                                       max_new_tokens=100,\n",
    "                                       pad_token_id=tokenizer.eos_token_id,\n",
    "                                       output_logits=True,\n",
    "                                       return_dict_in_generate=True)\n",
    "        generated_sentence= tokenizer.decode(generated_ids.sequences[:, eencoded_context.shape[-1]:][0], skip_special_tokens=True)\n",
    "        if model_name in ['WizardLM-2-7B','WizardLM-7B']:\n",
    "            first_user_index = generated_sentence.find('USER')\n",
    "    \n",
    "            # 첫 번째 'USER'가 등장하기 전까지의 부분 문자열 추출\n",
    "            generated_sentence = generated_sentence[:first_user_index]\n",
    "        \n",
    "        #print(generated_ids)\n",
    "        #generated_sentence= tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        generated_sentences.append(generated_sentence)\n",
    "        print(\"########GENERATED:########\")\n",
    "        print(generated_sentence)\n",
    "        print(\"########Generation 끝#### \")\n",
    "    \n",
    "    data['generated'] =generated_sentences\n",
    "    data['Emotion'] = input_label\n",
    "    with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,emotion),\"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b484a4e-9fcf-4e70-8c92-147b15d86a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name in ['DialoGPT-large', 'GPT-2-large','WizardMath-7B','ChatGLM-6B']:\n",
    "    def get_generated_sentences_data(attribute, sys_prompt,data_num,emotion,input_label):\n",
    "        \n",
    "        data = prompt_make(attribute,sys_prompt)\n",
    "        data = data.sample(frac=1,random_state = 1).reset_index(drop=True)[:data_num]\n",
    "        generated_sentences_masking_attribute = []\n",
    "        generated_sentences = []\n",
    "        \n",
    "        for i in tqdm(range(len(data))):\n",
    "            print(data['prompt'][i])\n",
    "            encoded_context = torch.tensor([tokenizer.encode(data['prompt'][i])]).to(device)\n",
    "            attention_mask = torch.ones(1,len(encoded_context[0])).to(device)\n",
    "            generated_ids = model.generate(encoded_context, max_length=256, do_sample =True,\n",
    "                                           attention_mask = attention_mask,\n",
    "                                           #pad_token_id=tokenizer.eos_token_id,\n",
    "                                           \n",
    "                                          )\n",
    "            generated_sentence= tokenizer.decode(generated_ids[:, encoded_context.shape[-1]:][0], skip_special_tokens=True)\n",
    "            generated_sentences.append(generated_sentence)\n",
    "            print(\"########GENERATED:########\")\n",
    "            print(generated_sentence)\n",
    "            print(\" \")\n",
    "            \n",
    "        data['generated'] =generated_sentences\n",
    "        data['Emotion'] = input_label\n",
    "        with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,emotion),\"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ee4601-115f-412b-b98a-58cd1a55102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"In this task you will be shown a conversation context. You need to generate a response to the conversation based on the context.\"\n",
    "data_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1643919-da31-42f2-bba5-91c033b03911",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 553/553 [00:00<00:00, 36729.22it/s]\n",
      "  0%|                                                                                  | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "In this task you will be shown a conversation context. You need to generate a response to the conversation based on the context.You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is joy.\n",
      "\n",
      "### Input:\n",
      "Mom , when you come home , please buy a coat , a skirt , a sweater ... </s>\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(attribute_name_list)):\n\u001b[1;32m     11\u001b[0m     attribute \u001b[38;5;241m=\u001b[39m attribute_name_list[i]\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mget_generated_sentences_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43memotions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mget_generated_sentences_data\u001b[0;34m(attribute, sys_prompt, data_num, emotion, input_label)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n\u001b[0;32m---> 11\u001b[0m     encoded_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m][i])])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(encoded_context[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(encoded_context, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, do_sample \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                                    attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m     15\u001b[0m                                    \u001b[38;5;66;03m#pad_token_id=tokenizer.eos_token_id,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                                    \n\u001b[1;32m     17\u001b[0m                                   )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "emotions = [\"joy\", \"surprise\", \"anger\", \"sadness\", \"disgust\", \"fear\",\"general\"]\n",
    "attribute_name_list = [\"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is joy.\",\n",
    "                       \"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is surprise.\",\n",
    "                      \"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is anger.\",\n",
    "                        \"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is sadness.\",\n",
    "                        \"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is disgust.\",                    \n",
    "                        \"You should generate an utterance that reflects the speaker's emotion given. Speaker's emotion is fear.\",\n",
    "                       \"\"\n",
    "                      ]\n",
    "for i in range(len(attribute_name_list)):\n",
    "    attribute = attribute_name_list[i]\n",
    "    get_generated_sentences_data(attribute, sys_prompt,data_num,emotions[i],input_label=i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c2b07d-bf6e-4912-ac82-d596a644290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9358f548-355c-4428-8875-7ebee0c54f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.classifier  = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768,class_num))\n",
    "    def forward(self,inputs):\n",
    "        bert_outputs = self.bert(**inputs,return_dict =True)\n",
    "        pooler_output = bert_outputs.last_hidden_state[:,0]\n",
    "        \n",
    "        logits = self.classifier(pooler_output)        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data =df \n",
    "        self.data_list, self.label_list = self.load_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    def load_data(self):\n",
    "        data_list = self.data['generated']\n",
    "        label_list = self.data['Emotion']\n",
    "        \n",
    "        return data_list, label_list \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data_list[index]\n",
    "        label = torch.tensor(self.label_list[index], dtype = torch.long)\n",
    "        return data, label\n",
    "\n",
    "def get_accuracy_by_bert(data,device,\n",
    "                         emotion,class_num\n",
    "                        ):\n",
    "    if emotion =='ALL':\n",
    "        bert_model = torch.load('/home/wooseok/layer_analysis_prompt_based/Bert_emotion_classifier/6classbert_model.pt')\n",
    "\n",
    "    else:\n",
    "        bert_model = torch.load('/home/wooseok/layer_analysis_prompt_based/Bert_emotion_classifier/2class/{}_bert_model.pt'.format(emotion))\n",
    "\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    max_len= 500\n",
    "    \n",
    "    data = data[['generated','Emotion']]\n",
    "    test_set = CustomDataset(data)\n",
    "    test_dl = DataLoader(test_set,batch_size=20, shuffle = False)    \n",
    "    \n",
    "    test_acc = 0.0\n",
    "    test_n_samples = 0\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(tqdm(test_dl, ncols=80, desc='valid_step')):\n",
    "            data, y_true = batch\n",
    "            data = bert_tokenizer(list(data), return_tensors='pt', padding=True, truncation=True)\n",
    "            \n",
    "            data = { k: v.to(device) for k, v in data.items() }\n",
    "            y_true = y_true.to(device)\n",
    "            all_y_true.extend(y_true.cpu().numpy())\n",
    "            \n",
    "            y_pred = bert_model(data)\n",
    "            y_preddd = y_pred.argmax(dim=1)\n",
    "            all_y_pred.extend(y_preddd.cpu().numpy())\n",
    "            y_prediction = y_pred.argmax(1)\n",
    "\n",
    "            \n",
    "            test_acc += torch.sum(y_prediction == y_true).item()\n",
    "            test_n_samples += len(y_true)\n",
    "    test_acc = (test_acc / test_n_samples) * 100.\n",
    "    #print(y_pred.argmax(1))\n",
    "    print(\"accuracy:\",test_acc)\n",
    "    return test_acc  ,all_y_true,all_y_pred,test_n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9186ef0-b216-4ab9-9990-e2aa7ea0e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 82.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 28.000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 32.0\n",
      " \n",
      "29.666666666666668\n",
      "49.333333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_num = 2       \n",
    "bert_model = Model()\n",
    "\n",
    "\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"general\"),\"rb\") as f:\n",
    "    data_general =pickle.load(f)\n",
    "\n",
    "binary_general_values =[]\n",
    "binary_controlled_values =[]\n",
    "for i in range(len(emotions)-1):\n",
    "    with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,emotions[i]),\"rb\") as f:\n",
    "        data_att =pickle.load(f)\n",
    "    data_att['Emotion']=0\n",
    "    data_general['Emotion']= 0\n",
    "    binary_general_values.append(get_accuracy_by_bert(data_general,device,emotions[i],class_num=2)[0])\n",
    "    binary_controlled_values.append(get_accuracy_by_bert(data_att,device,emotions[i],class_num=2)[0]) \n",
    "with open(\"최종results_{}/accuracy_binary_general.pkl\".format(model_name),\"wb\") as f:\n",
    "        pickle.dump(binary_general_values, f)\n",
    "with open(\"최종results_{}/accuracy_binary_controlled.pkl\".format(model_name),\"wb\") as f:\n",
    "        pickle.dump(binary_controlled_values, f)\n",
    "\n",
    "print(\" \")\n",
    "print(np.mean(binary_general_values))\n",
    "print(np.mean(binary_controlled_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b09d99-613e-4d8c-a834-96a2a4b4a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 56.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_step: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 40.0\n",
      "37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_num = 6\n",
    "bert_model = Model()\n",
    "\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"joy\"),\"rb\") as f:\n",
    "        data_joy = pickle.load(f)\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"surprise\"),\"rb\") as f:\n",
    "        data_surprise=pickle.load(f)\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"anger\"),\"rb\") as f:\n",
    "        data_anger=pickle.load(f)\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"sadness\"),\"rb\") as f:\n",
    "        data_sadness=pickle.load(f)\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"disgust\"),\"rb\") as f:\n",
    "        data_disgust=pickle.load(f)\n",
    "with open(\"최종results_{}/generatedSentence_{}.pkl\".format(model_name,\"fear\"),\"rb\") as f:\n",
    "        data_fear=pickle.load(f)\n",
    "\n",
    "data_joy['Emotion']=0\n",
    "data_surprise['Emotion']=1\n",
    "data_anger['Emotion']=2\n",
    "data_sadness['Emotion']=3\n",
    "data_disgust['Emotion']=4\n",
    "data_fear['Emotion']=5\n",
    "\n",
    "multi_controlled_values =[]\n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_joy,device,'ALL',6)[0])  # 0 \n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_surprise,device,'ALL',6)[0]) #1\n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_anger,device,'ALL',6)[0]) # 2\n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_sadness,device,'ALL',6)[0]) # 3 \n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_disgust,device,'ALL',6)[0]) # 4 \n",
    "multi_controlled_values.append(get_accuracy_by_bert(data_fear,device,'ALL',6)[0]) # 5\n",
    "with open(\"최종results_{}/accuracy_multi_controlled.pkl\".format(model_name),\"wb\") as f:\n",
    "        pickle.dump(multi_controlled_values, f)\n",
    "print(np.mean(multi_controlled_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b9091-7cbf-481e-b689-05da7acaebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713feaa6-2256-4c3f-be80-3880bbb5700a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
